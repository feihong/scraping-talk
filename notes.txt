Web Scraping: An Overview
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Organization
============

What I mean when I say scraper
Brief overview of the main types of scrapers
Components of a scraping system
Pro tips for scraping enthusiasts

What I mean when I say scraper
==============================

When I say "scraper", I really mean any program that retrieves data from a source on the web, and imports it in a format that can be used on another system.

The difference between "scraper" and "importer" is not so meaningful, especially when it comes to government data.

A logically laid out web page might be easier to deal with than a weirdly-formatted CSV file.

Main types of scrapers
======================
CSV
RSS/Atom
JSON
XML
HTML crawler
Web browster
PDF
Database dump
GIS dump
Mixed

CSV
===

import csv

- You should usually use csv.DictReader. Your code will be much more readable.
- There are times when you don't want to use csv.DictReader. If the column names are all caps, consider making them lowercase.

def get_rows(csv_file):
    reader = csv.reader(open(csv_file))
    # Get the column names, lowercased.
    column_names = tuple(k.lower() for k in reader.next())
    for row in reader:
        yield dict(zip(column_names, row))

- Some CSV datasets will not have the same number of items on each row. For example: the Atlanta crime dataset.

JSON
====

import json

XML
===

import lxml.etree

- Get rid of namespaces in the input document. http://bit.ly/LO5x7H
- A lot of XML-based datasets have a fairly flat structure that looks something like this:

<root>
    <items>
        <item>
            <name>Frodo Samwise</name>
            <age>56</age>
            <occupation>Tolkien scholar</occupation>
            <description>Short, with hairy feet</description>
        </item>
        ...
    </items>
</root>

In cases like this, I always convert the XML elements to dictionaries:

import lxml.etree
tree = lxml.etree.fromstring(SOME_XML_STRING)
for el in tree.findall('items/item'):
    children = el.getchildren()
    keys = (c.tag for c in children)
    values = (c.text for c in children)
    yield dict(zip(keys, values))

HTML
====
import requests
import lxml.html

- I generally use XPath, but I'd give pyquery a shot.
- In the rare case when the HTML is so weird that lxml.html's parser can't handle it, use html5lib (you can still create the tree object in lxml).
- In some cases, the data can be scraped from a chunk of JavaScript embedded in the page. This can save you a ton of time since the json module does all the parsing for you.
- If there is a lot of navigation involved, consider using mechanize. However, I've found that mechanize can really choke on wacky government websites.

Web browster
============

This is usually only for cases where part of the page is generated by JavaScript. There are plenty of government sites like this, but they often are not worth the effort to scrape.

Although I wrote PunkyBrowster, I can't really recommend it over ghost.py. It seems to have a better API, supports PySide as well as Qt, and has a more permissive license (MIT).

PDF
===

- Not as hard as it looks.
- There aren't any Python libraries that handle all kinds of PDF documents.
- Use the pdftohtml command from poppler-utils, and have it convert the PDF to XML.
- For debugging, use pdftohtml to generate HTML and inspect it in your browser.
- If the text in the PDF is in tabular format, you can group the text cells into regions and determine the structure via column position.

Example: Pittsburgh police blotter.

RSS/Atom
========

- import feedparser.
- Sometimes feedparser can't handle custom fields. In those cases you'll have to fall back to parsing the XML.
- A lot of RSS feeds are not compliant XML. Consider using html5lib if feedparser fails.

Database dump
=============

- Might come in the form of a Microsoft Access file (use mbtools), or a series of CSV files (each file representing all the data for a particular table).
- You should load the data into an SQLite database and run your queries on it. Not very efficient, but will save you a lot of time.

GIS dump
========

I've never really worked much with shape files or KML files. Generally if the provide this format they will offer other options as well.

Mixed
=====
This is very common. Some organization offers a CSV download, but there's no API so you have to scrape their web page to find the link for it.

Example: Atlanta crime dataset.

Parts of a scraper program
==========================
Downloader
Cache system
Raw item retriever
Existing item detector
Item transformer
Reporting system

Cache System
============
Pretty print downloaded JSON and XML to increase readability when debugging.

Steps to writing a scraper program
==================================
Find the data source
Find the metadata
Verifying the primary key
Developing
Debugging
Fixing
More fixing

An essential tool for writing HTML scrapers
===========================================

Firefinder

Test out

Storing scraped data
====================

- Don't make your tables before you understand how you will use the data.
We never created new tables to store scraped data.
We had a set of tables that could be repurposed for most datasets: schema, schema field, newsitem, attribute.
Consider using ZODB to store the data.

Design patterns for scraping
============================

Lookup pattern
Scraper superclass that encapsulates the main scraper logic.

Working with government data sources
====================================
- Some government data sources are only available at certain times of day.
- Be careful about rate limiting or IP blocking.
- If your data was scraped from a web page, think twice about doing something with the aggregate data. It often contains holes, sometimes purposefully.
- When all else fails, give the appropriate government office a call. And if you actually manage to connect with an actual person, KEEP their contact information.
- The EveryBlock source code contains a lot of emails and phone numbers for various government employees.

Advice to fellow scraping enthusiasts
=====================================

- In the rare case when you can't know in advance what encoding the page is in, use charade, not chardet.
- Remember to clean any HTML that you intend to display to users.
- If the database doesn't allow filtering by date, then it's probably a lost cause (unless you only care about historical data).
- Pick your geocoder carefully. For example, Google Maps's geocoder stipulates that you must show any geocoded result on a Google Map.
- If your scraper fails, do NOT fix it. If someone actually complains, consider fixing it.
